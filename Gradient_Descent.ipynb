{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0qjFiywpPBA"
   },
   "source": [
    "# Les bibliothéques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "JaP9_91jpIB_"
   },
   "outputs": [],
   "source": [
    "from sympy import *\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits import mplot3d\n",
    "from plotly.subplots import make_subplots\n",
    "from numpy.linalg import norm\n",
    "import time\n",
    "from decimal import *\n",
    "import plotly.figure_factory as ffactory\n",
    "from copy import deepcopy\n",
    "\n",
    "x=Symbol(\"x\")\n",
    "y=Symbol(\"y\")\n",
    "p=Symbol(\"p\")\n",
    "init_printing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNdL0tGzQeTa"
   },
   "source": [
    "# Les fonctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ENRHploymea"
   },
   "source": [
    "### Choisir la fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "DX1LM7CIxAtM"
   },
   "outputs": [],
   "source": [
    "def localFunctions():\n",
    "  functionsList=['(1 - x)² + 100 * (y - x²)²', 'x * exp^(-x² - y²)', 'x² + y²']\n",
    "  functionsListValue=['(1 - x)**2 + 100 * (y - x**2)**2', 'x*exp(-x**2-y**2)', 'x**2 + y**2']\n",
    "  fctDisplay=widgets.Select(\n",
    "      options=functionsListValue,\n",
    "      description='Choisir une fonction:',\n",
    "      disabled=False\n",
    "  )\n",
    "  return fctDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "xeal_aPi3YaZ"
   },
   "outputs": [],
   "source": [
    "def givenFunctions():\n",
    "  #fctinput=widgets.Text(\n",
    "  #  value='x',\n",
    "  #  placeholder='Tapez une fonction',\n",
    "  #  description='Tapez une fonction:'\n",
    "  #  disabled=False\n",
    "#)\n",
    "\n",
    "  #global variables\n",
    "  #variables=[]\n",
    "  #nbvar=0\n",
    "  #while(nbvar==0):\n",
    "  #  nbvar=input(\"Tapez le nombre des variables de la fonction = \")\n",
    "  #  if(not(nbvar.isnumeric()) ):\n",
    "  #    print(\"\\t ⚠️\\033[91m Veuillez choisir un entier positif \\033[0;37m \\n\")\n",
    "  #    nbvar=0\n",
    "  #  else:\n",
    "   #   nbvar=int(nbvar)\n",
    "  #    if ((nbvar<=0)):\n",
    "  #      print(\"\\t ⚠️\\033[91m Veuillez choisir un entier positif et supérieur à zéro \\033[0;37m \\n\")\n",
    "  #      nbvar=0\n",
    "  #print(\"\\n\")\n",
    "\n",
    "  #for i in range(nbvar):\n",
    "  #  var=input(\"Tapez la variable N° \"+ str(i+1) +\" = \")\n",
    "  #  while (var.isnumeric()):\n",
    "  #    print(\"\\t ⚠️\\033[91m Veuillez choisir un caractère \\033[0;37m \\n\")\n",
    "  #    var=input(\"Tapez la variable N° \"+ str(i+1)+\" = \")\n",
    "  #  variables.append(Symbol(var))\n",
    "  #print(\"Les variables sont : \", variables)\n",
    "\n",
    "  fctinput = input(\"Tapez une fonction : \")\n",
    "  return fctinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "GHNKejrX5S2x"
   },
   "outputs": [],
   "source": [
    "def restrictionInputFunction():\n",
    "  print(\"\\t ⚠️ \\033[91m Attention:\\033[0;37m\")\n",
    "  print(\"\\t - une fonction peut contenir seulement deux variables '\\033[0;32m x \\033[0;37m' et '\\033[0;32m y \\033[0;37m' \")\n",
    "  print(\"\\t - Les fonction trigonometriques doivent étre sous la forme:\")\n",
    "  print(\"\\t     * sinus(a)        -> \\033[0;32m sin(a) \\033[0;37m               \")\n",
    "  print(\"\\t     * cosinus(a)      -> \\033[0;32m cos(a) \\033[0;37m                \")  \n",
    "  print(\"\\t     * tangente(a)     -> \\033[0;32m tan(a) \\033[0;37m               \")\n",
    "  print(\"\\t     * arcsinus(a)     -> \\033[0;32m arcsin(a) \\033[0;37m \")\n",
    "  print(\"\\t     * arccosinus(a)   -> \\033[0;32m arccos(a) \\033[0;37m               \")\n",
    "  print(\"\\t     * arctangente(a)  -> \\033[0;32m arctan(a) \\033[0;37m                \")\n",
    "  print(\"\\t - La fonction exponentiel doit étre sous la forme:\")\n",
    "  print(\"\\t     * ₑa              -> \\033[0;32m exp(a) \\033[0;37m               \")\n",
    "  print(\"\\t - La fonction logarithmique doit étre sous la forme:\")\n",
    "  print(\"\\t     * log(a)          -> \\033[0;32m log(a) \\033[0;37m              \")\n",
    "  print(\"\\t - Les fonctions arithmétiques doivent étre sous la forme:\")\n",
    "  print(\"\\t     * √a              -> \\033[0;32m sqrt(a) \\033[0;37m                \")\n",
    "  print(\"\\t     * a^b             -> \\033[0;32m a**b \\033[0;37m              \")\n",
    "  print(\"\\t - La fonction valeur absolue doit étre sous la forme:\")\n",
    "  print(\"\\t     * |a|             -> \\033[0;32m abs(a) \\033[0;37m             \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "1RsqoyrkEhIt"
   },
   "outputs": [],
   "source": [
    "def restrictionRespected(fct):\n",
    "  #x=symbols(\"x\")\n",
    "  #y=symbols(\"y\")\n",
    "  try:\n",
    "    z=eval(fct, {\"x\": x, \"y\": y, 'pi':pi, 'sqrt': sqrt, 'exp':exp, 'sin':sin, 'tan':np.tan, 'cos':cos, 'asin':asin, 'atan':atan, 'acos':acos, 'pow':pow, 'abs':abs, 'ln':log})\n",
    "    z=sympify(fct)\n",
    "    print(z.diff(x))\n",
    "    return 1\n",
    "  except Exception as e:\n",
    "    print(\"\\t ⚠️ \\033[91m Attention La fonction saisite est incorrecte \\033[0;37m\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lr9T43jCygVu"
   },
   "source": [
    "### Affichage des graphes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "dWCCmlX8pImA"
   },
   "outputs": [],
   "source": [
    "def ff(ax,ay , function):\n",
    "  #print(\"func\",function)\n",
    "  #return  eval(function,{'__builtins__': None}, {\"x\": x, \"y\": y, 'pi':np.pi, 'sqrt': np.sqrt, 'exp':np.exp, 'sin':np.sin, 'tan':np.tan, 'cos':np.cos, 'asin':asin, 'atan':atan, 'acos':acos, 'pow':pow, 'abs':abs, 'ln':log})\n",
    "  fct=sympify(function)\n",
    "  #print(\"eval = \",eval(function,{\"x\":ax,\"y\":ay,'exp':np.exp}))\n",
    "  func = lambdify([x,y], fct,modules='numpy')\n",
    "  return func(ax,ay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "58vSQ16Cin7c"
   },
   "outputs": [],
   "source": [
    "def f(ax,ay , function):\n",
    "  #print(\"func\",function)\n",
    "  #return  eval(function,{'__builtins__': None}, {\"x\": x, \"y\": y, 'pi':np.pi, 'sqrt': np.sqrt, 'exp':np.exp, 'sin':np.sin, 'tan':np.tan, 'cos':np.cos, 'asin':asin, 'atan':atan, 'acos':acos, 'pow':pow, 'abs':abs, 'ln':log})\n",
    "  #fct=sympify(function)\n",
    "  #print(\"eval = \",eval(function,{\"x\":ax,\"y\":ay,'exp':np.exp}))\n",
    "  #func = lambdify([x,y], fct,modules='numpy')\n",
    "  #print(\"func(1,1)\",func(1,1))\n",
    "  func = eval(\"lambda x,y: \"+str(function))\n",
    "  return func(ax,ay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "0c_KubY-nEm4"
   },
   "outputs": [],
   "source": [
    "def plotFunction(function):\n",
    "  x_feature = np.linspace(-10,10,20)\n",
    "  y_feature = np.linspace(-10,10,20)\n",
    "  # Creating 2-D grid of features\n",
    "  X, Y = np.meshgrid(x_feature, y_feature)\n",
    "  Z = ff(X,Y,function)\n",
    "  fig = go.Figure(data=[go.Surface(x=x_feature, y=y_feature, z=Z)])\n",
    "  fig.update_layout(title='Tracage de courbe de la fonction: '+function, autosize=False,\n",
    "                  width=700, height=700,\n",
    "                  margin=dict(l=65, r=50, b=65, t=90))\n",
    "\n",
    "  fig.update_layout(\n",
    "      template=\"plotly_dark\",\n",
    "      margin=dict(r=10, t=25, b=40, l=60),\n",
    "      \n",
    "  )\n",
    "\n",
    "  fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "yIsF_eq5wavn"
   },
   "outputs": [],
   "source": [
    "def plotFunctionWithCounter(function):\n",
    "  \n",
    "  fig = make_subplots(rows=1, cols=2, subplot_titles=('Tracage de courbe de la fonction avec la ligne de niveau',\n",
    "                                                          'Ligne de niveau de la fonction'),specs=[[{\"type\": \"surface\"},{'type': 'xy'}],\n",
    "                                                                                      ])\n",
    "  x_feature = np.linspace(-10,10,20)\n",
    "  y_feature = np.linspace(-10,10,20)\n",
    "    \n",
    "  # Creating 2-D grid of features\n",
    "  X, Y = np.meshgrid(x_feature, y_feature)\n",
    "    \n",
    "  Z = ff(X,Y,function)\n",
    "  fig.add_trace(go.Surface(x=x_feature, y=y_feature, z=Z,name=\"Plot\",contours_z=dict(show=True, usecolormap=True,\n",
    "                                    highlightcolor=\"limegreen\", project_z=True)), 1, 1)\n",
    "  fig.add_trace(go.Contour(\n",
    "        x=x_feature, y=y_feature, z=Z,\n",
    "        contours=dict(\n",
    "            coloring='lines',\n",
    "            showlabels=True,)\n",
    "    ), 1, 2)\n",
    "  fig.update_layout(title={\n",
    "          'text': \"Fonction:\"+function,\n",
    "          'y':0.03,\n",
    "          'x':0.03,\n",
    "          'xanchor': 'left',\n",
    "          'yanchor': 'bottom'})\n",
    "\n",
    "\n",
    "  fig.update_layout(\n",
    "      template=\"plotly_dark\",\n",
    "      margin=dict(r=10, t=25, b=40, l=60),\n",
    "      \n",
    "  )\n",
    "  fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kplR0FfyZkY"
   },
   "source": [
    "### Calcule du vecteur gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "QKukJoRUyv62"
   },
   "outputs": [],
   "source": [
    "def partial_derivative(element, fnc):\n",
    "\t\"\"\"\n",
    "\tpartial : sympy.core.symbol.Symbol * sympy.core.add.Add -> sympy.core.add.Add\n",
    "\tpartial(element, function) \n",
    "  Performs partial derivative of a function of several variables is its derivative with respect to one of those variables, with the others held constant. Return partial_diff.\n",
    "\t\"\"\"\n",
    "\treturn fnc.diff(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "B-XPsqUAzB8Q"
   },
   "outputs": [],
   "source": [
    "init_printing()\n",
    "\n",
    "def gradient(partials):\n",
    "\t\"\"\"\n",
    "\tgradient : List[sympy.core.add.Add] -> numpy.matrix\n",
    "\tgradient(partials) Transforms a list of sympy objects into a numpy matrix. Return grad.\n",
    "\t\"\"\"\n",
    "\treturn Matrix([[partials[0]], [partials[1]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "dr90rY2gyZ95"
   },
   "outputs": [],
   "source": [
    "def firstDerivation(symbols_list, fnc):\n",
    "  \n",
    "  partials = []\n",
    "  # loop throught the elements of symboles\n",
    "  for element in symbols_list:\n",
    "    # do the partial derivation of each symbole\n",
    "    partials.append(partial_derivative(element, fnc))\n",
    "  return partials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "CIemBcqMgBxQ"
   },
   "outputs": [],
   "source": [
    "def calculGradientVector(function):  \n",
    "  global grad\n",
    "  # get partial for each symbole\n",
    "  partials = firstDerivation([x, y],sympify(function))\n",
    "  # get the gradient vector\n",
    "  grad = gradient(partials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MY4kfLWFn6rb"
   },
   "source": [
    "### Calcule du matrice Hessienne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "VfOL_QVJDcye"
   },
   "outputs": [],
   "source": [
    "def hessian(partials_second, cross_derivatives):\n",
    "\t\"\"\"\n",
    "\thessian : List[sympy.core.add.Add] * sympy.core.add.Add -> numpy.matrix\n",
    "\thessian(partials_second, cross_derivatives) Transforms a list of sympy objects into a numpy hessian matrix. Return hessianmat.\n",
    "\t\"\"\"\n",
    "\thessianmat = Matrix([[partials_second[0], cross_derivatives[0]], [cross_derivatives[1], partials_second[1]]])\n",
    "\n",
    "\treturn hessianmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "2HDVCL5z44Bn"
   },
   "outputs": [],
   "source": [
    "# second derivation\n",
    "def secondDerivation(symbols_list, partials):\n",
    "\tpartials_second = []\n",
    "  # second derivation\n",
    "\tfor i in range(0, len(symbols_list)):\n",
    "\t\tpartial_diff = partial_derivative(symbols_list[i], partials[i])\n",
    "\t\tpartials_second.append(partial_diff)\n",
    "\treturn partials_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "_otJoDWYoB3U"
   },
   "outputs": [],
   "source": [
    "def calculHassienneMatrix(function):\t# get partial for each symbole\n",
    "  partials = firstDerivation([x,y],sympify(function))\n",
    "  cross_derivatives=[]\n",
    "  # derivation de y % x ( ie: 2*y % x)\n",
    "  cross_derivatives.append(partial_derivative(x, partials[1]))\n",
    "  cross_derivatives.append(partial_derivative(y, partials[0]))\n",
    "  # second derivation\n",
    "  partials_second = secondDerivation([x,y], partials)\n",
    "  \n",
    "  # get the hessian matrix\n",
    "  hessianmat = hessian(partials_second, cross_derivatives)\n",
    "  # display\n",
    "  print(\"La matrice Hessian de la fonction {0} est :\".format(function))\n",
    "  pprint(hessianmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saOeKLIsfPTH"
   },
   "source": [
    "## Méthode de gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDZpCK48fPbK"
   },
   "source": [
    "### à pas fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ohFTIabfPnK"
   },
   "outputs": [],
   "source": [
    "def getParameters():\n",
    "  global eps \n",
    "  global X0_0\n",
    "  global learning_rate\n",
    "  testEPS=1\n",
    "  testLearning_rate=1\n",
    "  X0_0=[]\n",
    "  # Lecture d'epsilone\n",
    "  while(testEPS):\n",
    "    try:\n",
    "        print(\"\\t ✅ \\033[0;32mRecommendation:\\033[0;37m\")\n",
    "        print(\"\\t Epsilon sert à symboliser un nombre ou une quantité extrêmement petite\")\n",
    "        eps = Decimal(input(\"\\t Tapez la valeur de epsilon = \"))\n",
    "        testEPS=0\n",
    "        if(eps<0):\n",
    "          print(\"\\t ⚠️\\033[91m Veuillez choisir un reél strictement positif\\033[0;37m \\n\")\n",
    "          testEPS=1\n",
    "    except ValueError:\n",
    "        print(\"\\t ⚠️\\033[91m Veuillez choisir un reél\\033[0;37m \\n\")\n",
    "    except InvalidOperation:\n",
    "        print(\"\\t ⚠️\\033[91m Veuillez choisir un reél\\033[0;37m \\n\")\n",
    "        \n",
    "  # Lecture de learning_rate\n",
    "  while(testLearning_rate):\n",
    "    try:\n",
    "        print(\"\\t ✅ \\033[0;32m\\Recommendation:\\033[0;37m\")\n",
    "        print(\"\\t Si le pas est trop grand, alors vous ferez de trop grands pas dans la descente de gradient. Cela a l’avantage de descendre rapidement vers le minimum de la fonction, mais vous risquez de louper ce minimum en oscillant autour à l’infini\")\n",
    "        print(\"\\t Si le pas très faible, alors vous risquez de mettre un temps infini avant de converger vers le minimum de la fonction.\")\n",
    "        learning_rate = Decimal(input(\"\\t Tapez la valeur de pas = \"))\n",
    "        testLearning_rate=0\n",
    "        if(learning_rate<=0):\n",
    "          print(\"\\t ⚠️\\033[91m Veuillez choisir un reél positif\\033[0;37m \\n\")\n",
    "          testLearning_rate=1\n",
    "    except ValueError:\n",
    "        print(\"\\t ⚠️\\033[91m Veuillez choisir un reél\\033[0;37m \\n\")\n",
    "    except InvalidOperation:\n",
    "        print(\"\\t ⚠️\\033[91m Veuillez choisir un reél\\033[0;37m \\n\")\n",
    "          \n",
    "   #Lecture de XO\n",
    "  X0_0=[0,0]\n",
    "  testX0=1\n",
    "  while(testX0):\n",
    "    try:\n",
    "      X0_0[0] = Decimal(input(\"\\t Tapez l’abscisse du vecteur de départ= \"))\n",
    "      testX0=0\n",
    "      if(not((str(X0_0[0]).strip('-')))):\n",
    "        print(\"\\t ⚠️\\033[91m Veuillez choisir une valeur entière\\033[0;37m \\n\")\n",
    "        testX0=1\n",
    "    except ValueError:\n",
    "        print(\"\\t ⚠️\\033[91m Veuillez choisir un reél\\033[0;37m \\n\")\n",
    "    except InvalidOperation:\n",
    "        print(\"\\t ⚠️\\033[91m Veuillez choisir un reél\\033[0;37m \\n\")\n",
    "  X0_0[0]=Decimal(X0_0[0])\n",
    "\n",
    "  testX01=1\n",
    "  while(testX01):\n",
    "    try:\n",
    "      X0_0[1] = Decimal(input(\"\\t Tapez ordonnée du vecteur de départ= \"))\n",
    "      testX01=0\n",
    "      if(not((str(X0_0[1]).strip('-')))):\n",
    "        print(\"\\t ⚠️\\033[91m Veuillez choisir une valeur entière\\033[0;37m \\n\")\n",
    "        testX01=1\n",
    "    except ValueError:\n",
    "        print(\"\\t ⚠️\\033[91m Veuillez choisir un reél\\033[0;37m \\n\")\n",
    "    except InvalidOperation:\n",
    "        print(\"\\t ⚠️\\033[91m Veuillez choisir un reél\\033[0;37m \\n\")\n",
    "  X0_0[1]=Decimal(X0_0[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "6hqts9z0Kwbf"
   },
   "outputs": [],
   "source": [
    "def printParameters():\n",
    "  print(\"\\n\")\n",
    "  print(\"- Les paramétres utilisé sont: \")\n",
    "  print(\"\\t Pas= \"+str(learning_rate))\n",
    "  print(\"\\t Le vecteur de départ X0= [\"+ str(X0_0[0])+\",\"+ str(X0_0[1])+\"]\")\n",
    "  print(\"\\t epsilon= \"+str(eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "mcpcAlRrOnUe"
   },
   "outputs": [],
   "source": [
    "def plotGradientDescent(xit,yit,zit,interationNumber):\n",
    "\n",
    "  x_feature = np.linspace(-10,10,20) # initialisation d'un tableau entre 10 et -10\n",
    "  y_feature = np.linspace(-10,10,20) # initialisation d'un tableau entre 10 et -10\n",
    "  # Creating 2-D grid of features\n",
    "  X, Y = np.meshgrid(x_feature, y_feature)\n",
    "\n",
    "  Z = ff(X,Y,function) # calcule de la fonction\n",
    "  trace1 = {\n",
    "  \"mode\": \"lines+markers\", \n",
    "  \"name\": \"Trace de Gradient Descent avec plot\", \n",
    "  \"hovertext\": interationNumber,\n",
    "  \"type\": \"scatter3d\", \n",
    "  \"x\":xit,\n",
    "  \"y\":yit,\n",
    "  \"z\":zit,\n",
    "  \"marker\": {\n",
    "    \"size\": 8, \n",
    "    \"color\": \"red\"\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "  trace2 = {\n",
    "  \"mode\": \"lines+markers\", \n",
    "  \"name\": \"Trace de Gradient Descent\", \n",
    "  \"hovertext\": interationNumber, \n",
    "  \"type\": \"scatter3d\", \n",
    "  \"x\":xit,\n",
    "  \"y\":yit,\n",
    "  \"z\":zit,\n",
    "  \"marker_colorscale\":'Viridis'\n",
    "}\n",
    "  fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Tracage de courbe de la fonction avec le chemin d'exécution\",\n",
    "                                                          \"Le chemin d'exécution de la fonction\"),specs=[[{\"type\": \"surface\"},{'type': 'surface'}],])\n",
    "  fig.add_trace(go.Surface(x=x_feature, y=y_feature, z=Z), 1, 1)\n",
    "  fig.add_trace(go.Scatter3d(trace1), 1, 1)\n",
    "  fig.add_trace(go.Scatter3d(trace2), 1, 2)\n",
    "\n",
    "\n",
    "\n",
    "  fig.update_layout(title={\n",
    "          'text': \"Fonction:\"+function,\n",
    "          'y':0.03,\n",
    "          'x':0.03,\n",
    "          'xanchor': 'left',\n",
    "          'yanchor': 'bottom'})\n",
    "\n",
    "  fig.update_layout(legend=dict(\n",
    "    yanchor=\"top\",\n",
    "    y=0.99,\n",
    "    xanchor=\"left\",\n",
    "    x=0.4\n",
    "))\n",
    "  fig.update_layout(\n",
    "      template=\"plotly_dark\",\n",
    "      margin=dict(r=10, t=25, b=40, l=60),\n",
    "      \n",
    "  )\n",
    "  fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "ETScwd1GQ5jE"
   },
   "outputs": [],
   "source": [
    "def plotGradientDescentTableFixed(nb_iterations_fixed,xit_fixed,yit_fixed,zit_fixed):\n",
    "  numIteration=list(range(nb_iterations_fixed+1))\n",
    "  data_matrix=[]\n",
    "  data_matrix.append([\"N° iteration\",\"X\",\"Y\",\"f(X,Y)\"])\n",
    "  for i in range(nb_iterations_fixed+1):\n",
    "    data_matrix.append([numIteration[i],xit_fixed[i],yit_fixed[i],zit_fixed[i]])         \n",
    "  colorscale = [[0, '#9A169E'],[.5, '#efeaef'],[1, '#ffffff']]\n",
    "  fig =  ffactory.create_table(data_matrix,colorscale)\n",
    "  fig.update_layout(\n",
    "      template=\"plotly_dark\",\n",
    "      title_text = 'Tableau d’itérations',\n",
    "      margin = {'t':50},\n",
    "  )\n",
    "  fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "QzhiZJTwOEHU"
   },
   "outputs": [],
   "source": [
    "def gradientWithFixedLearningRateCalcul(function):\n",
    "  # initialisation\n",
    "  global X0\n",
    "  global xit_fixed\n",
    "  global yit_fixed\n",
    "  global zit_fixed\n",
    "  global iterationsWithFixedLR # liste des ittérations\n",
    "  global nb_iterations_fixed # nb d'ittération\n",
    "  X0=deepcopy(X0_0)\n",
    "  xit_fixed=[] # liste des abscisses\n",
    "  yit_fixed=[] # liste des ordonnées\n",
    "  zit_fixed=[] # list des résultats de la fonction après remplacement par le vecteur calculer\n",
    "  interationNumber=[] # initialiser la liste du nb d'ittérations\n",
    "\n",
    "  calculGradientVector(function) # calcule du vecteur gradient\n",
    "  iterationsWithFixedLR=[] # initialisation  de la liste des ittérations\n",
    "  nb_max_iterations = 3000 # Nb max d'iteration\n",
    "  nb_iterations_fixed = 0  #initialisation du nbre d'itération\n",
    "  #print(\"--------------------------------------------------------------------------------------------\")\n",
    "  #print(\"|     N° iteration     |          X           |          Y           |       f(X,Y)         |\")\n",
    "  #print(\"--------------------------------------------------------------------------------------------\")\n",
    "  Z0 = Decimal(str((f(X0[0],X0[1],function)))) # calcule de la fonction en remplçant x et y par le vecteur départ\n",
    "  iterationsWithFixedLR.append([X0[0],X0[1]]) # sauvegarder la valeur du point de départ\n",
    "  interationNumber.append(\"Trace \"+str(nb_iterations_fixed)) # sauvegarder la trace\n",
    "  #print(\"|\"+\" \"*((22-len(str(nb_iterations_fixed)))//2)+\"\"+str(nb_iterations_fixed)+\" \"*(22-len(str(nb_iterations_fixed))-(22-len(str(nb_iterations_fixed)))//2)+\"|\"+\" \"*((22-len(str(X0[0])))//2)+\"\"+str(X0[0])+\" \"*(22-len(str(X0[0]))-(22-len(str(X0[0])))//2)+\"|\"+\" \"*((22-len(str(X0[1])))//2)+\"\"+str(X0[1])+\" \"*(22-len(str(X0[1]))-(22-len(str(X0[1])))//2)+\"|\"+\" \"*((22-len(str(Z0)))//2)+\"\"+str(Z0)+\" \"*(22-len(str(Z0))-(22-len(str(Z0)))//2)+\"|\")\n",
    "  xit_fixed.append(X0[0]) # sauvegarder l'abscisse du point\n",
    "  yit_fixed.append(X0[1]) # sauvegarder l'ordonné du point\n",
    "  zit_fixed.append(Z0) # sauvegarder le résultat de la fonction en remplçant x et y par le vecteur départ\n",
    "  \n",
    "  X1=[0,0]\n",
    "  # xk+1=xk+alpha*dk\n",
    "  X1[0] = Decimal(str(X0[0])) - Decimal(str(learning_rate)) * Decimal(str(f(X0[0],X0[1],grad[0]))) # calcule de l'abscisse du nouveau vecteur\n",
    "  X1[1] = Decimal(str(X0[1])) - Decimal(str(learning_rate)) * Decimal(str(f(X0[0],X0[1],grad[1]))) # calcule de l'ordonnée du nouveau vecteur\n",
    "  Z0_tmp = Decimal(str(f(X1[0],X1[1],function))) # calcule de la fonction avec le nouveau vecteur \n",
    "  nb_iterations_fixed = nb_iterations_fixed + 1 # incrémenter le nbr d'ittératioon\n",
    "  \n",
    "  iterationsWithFixedLR.append([X1[0],X1[1]]) # sauvergarder la nouvelle valeur du vecteur\n",
    "  xit_fixed.append(X1[0]) # sauvegarder l'abscisse du point\n",
    "  yit_fixed.append(X1[1]) # sauvegarder l'ordonné du point\n",
    "  zit_fixed.append(Z0_tmp) # sauvegarder le résultat de la fonction en remplçant x et y par le nouveau vecteur Xk+1\n",
    "\n",
    "\n",
    "  interationNumber.append(\"Trace \"+str(nb_iterations_fixed)) # sauvegarder la trace\n",
    "  #print (\"nb_iter \"+str(nb_iterations_fixed)+\" = \") # affichage de nb d'ittération avec les coordonnées du vecteur\n",
    "  #pprint(Matrix([[X1[0]], [X1[1]]]))\n",
    "  #print(\"|\"+\" \"*((22-len(str(nb_iterations_fixed)))//2)+\"\"+str(nb_iterations_fixed)+\" \"*(22-len(str(nb_iterations_fixed))-(22-len(str(nb_iterations_fixed)))//2)+\"|\"+\" \"*((22-len(str(X1[0])))//2)+\"\"+str(X1[0])+\" \"*(22-len(str(X1[0]))-(22-len(str(X1[0])))//2)+\"|\"+\" \"*((22-len(str(X1[1])))//2)+\"\"+str(X1[1])+\" \"*(22-len(str(X1[1]))-(22-len(str(X1[1])))//2)+\"|\"+\" \"*((22-len(str(Z0_tmp)))//2)+\"\"+str(Z0_tmp)+\" \"*(22-len(str(Z0_tmp))-(22-len(str(Z0_tmp)))//2)+\"|\")\n",
    "  # ||Z0_tmp - Z0||² > eps signifie si la différence entre les deux points < epsilone ce qui implique que le fct a convergé\n",
    "  # vérifier si la ||Z0_tmp - Z0||² et si on a dépassé le nombre d'ittération maximale\n",
    "  try:\n",
    "    while ((norm(Z0_tmp - Z0) > eps) and (nb_iterations_fixed < nb_max_iterations)):\n",
    "      X0[0]=Decimal(X1[0]) # sauvegarder l'ancienne valeur de l'abscisse\n",
    "      X0[1]=Decimal(X1[1]) # sauvegarder l'ancienne valeur de l'ordonné\n",
    "      Z0=Z0_tmp # sauvegarder le résultat de la fonction avec l'ancienne valeur du vecteur \n",
    "\n",
    "      # xk+1=xk+alpha*dk\n",
    "      X1[0] = X0[0] - Decimal(str(learning_rate)) * Decimal(str(f(X0[0],X0[1],grad[0] ))) # calcule de l'abscisse du nouveau vecteur Xk+1\n",
    "      X1[1] = X0[1] - Decimal(str(learning_rate)) * Decimal(str(f(X0[0],X0[1],grad[1] ))) # calcule de l'ordonnée du nouveau vecteur Xk+1\n",
    "      \n",
    "      \n",
    "      Z0_tmp = Decimal(f(X1[0],X1[1],function)) # calcule de la fonction avec le nouveau vecteur \n",
    "      nb_iterations_fixed = nb_iterations_fixed + 1 # incrémenter le nbr d'ittératioon\n",
    "      iterationsWithFixedLR.append([X1[0],X1[1]]) # sauvergarder la nouvelle valeur du vecteur\n",
    "\n",
    "      interationNumber.append(\"Trace \"+str(nb_iterations_fixed)) # sauvegarder la trace\n",
    "\n",
    "      xit_fixed.append(Decimal(X1[0])) # sauvegarder l'abscisse du point\n",
    "      yit_fixed.append(Decimal(X1[1])) # sauvegarder l'ordonné du point\n",
    "      zit_fixed.append(Decimal(Z0_tmp)) # sauvegarder le résultat de la fonction en remplçant x et y par le nouveau vecteur Xk+1\n",
    "      #print(\"|\"+\" \"*((22-len(str(nb_iterations_fixed)))//2)+\"\"+str(nb_iterations_fixed)+\" \"*(22-len(str(nb_iterations_fixed))-(22-len(str(nb_iterations_fixed)))//2)+\"|\"+\" \"*((22-len(str(X1[0])))//2)+\"\"+str(X1[0])+\" \"*(22-len(str(X1[0]))-(22-len(str(X1[0])))//2)+\"|\"+\" \"*((22-len(str(X1[1])))//2)+\"\"+str(X1[1])+\" \"*(22-len(str(X1[1]))-(22-len(str(X1[1])))//2)+\"|\"+\" \"*((22-len(str(Z0_tmp)))//2)+\"\"+str(Z0_tmp)+\" \"*(22-len(str(Z0_tmp))-(22-len(str(Z0_tmp)))//2)+\"|\")\n",
    "\n",
    "  except :\n",
    "      print(\"\\t ⚠️\\033[91m Les valeurs de f sont très grandes\\033[0;37m \\n\")\n",
    "      print(\"\\t ✅ \\033[0;32m\\Recommendation:\\033[0;37m Tapez un pas petit\")\n",
    "      gradientWithFixedLearningRate()\n",
    "  #print(\"--------------------------------------------------------------------------------------------\")\n",
    "  plotGradientDescentTableFixed(nb_iterations_fixed,xit_fixed,yit_fixed,zit_fixed)\n",
    "  plotGradientDescent(xit_fixed,yit_fixed,zit_fixed,interationNumber) # affichage du graphe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "jzdiHbJu3oxC"
   },
   "outputs": [],
   "source": [
    "def gradientWithFixedLearningRate():\n",
    "  getParameters()\n",
    "  printParameters()\n",
    "  gradientWithFixedLearningRateCalcul(function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBNqM9ruYsZb"
   },
   "source": [
    "### pas optimale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpDwDK02_5ze"
   },
   "outputs": [],
   "source": [
    "def getParametersOp():\n",
    "  global eps \n",
    "  global X0_0\n",
    "  testEPS=1\n",
    "  X0_0=[]\n",
    "  # Lecture d'epsilone\n",
    "  while(testEPS):\n",
    "    try:\n",
    "        print(\"\\t ✅ \\033[0;32mRecommendation:\\033[0;37m\")\n",
    "        print(\"\\t Epsilon sert à symboliser un nombre ou une quantité extrêmement petite\")\n",
    "        eps = Decimal(input(\"\\t Tapez la valeur de epsilon = \"))\n",
    "        testEPS=0\n",
    "        if(eps<0):\n",
    "          print(\"\\t ⚠️\\033[91m Veuillez choisir un reél positif\\033[0;37m \\n\")\n",
    "          testEPS=1\n",
    "    except ValueError:\n",
    "        print(\"\\t ⚠️\\033[91m Veuillez choisir un reél\\033[0;37m \\n\")\n",
    "         \n",
    "   #Lecture de XO\n",
    "  X0_0=[0,0]\n",
    "  testX0=1\n",
    "  while(testX0):\n",
    "    try:\n",
    "      X0_0[0] = Decimal(input(\"\\t Tapez l’abscisse du vecteur de départ= \"))\n",
    "      testX0=0\n",
    "      if(not((str(X0_0[0]).strip('-')))):\n",
    "        print(\"\\t ⚠️\\033[91m Veuillez choisir une valeur entière\\033[0;37m \\n\")\n",
    "        testX0=1\n",
    "    except ValueError:\n",
    "        print(\"\\t ⚠️\\033[91m Veuillez choisir un reél\\033[0;37m \\n\")\n",
    "    except InvalidOperation:\n",
    "        print(\"\\t ⚠️\\033[91m Veuillez choisir un reél\\033[0;37m \\n\")\n",
    "  X0_0[0]=Decimal(X0_0[0])\n",
    "\n",
    "  testX01=1\n",
    "  while(testX01):\n",
    "    try:\n",
    "      X0_0[1] = Decimal(input(\"\\t Tapez ordonnée du vecteur de départ= \"))\n",
    "      testX01=0\n",
    "      if(not((str(X0_0[1]).strip('-')))):\n",
    "        print(\"\\t ⚠️\\033[91m Veuillez choisir une valeur entière\\033[0;37m \\n\")\n",
    "        testX01=1\n",
    "    except ValueError:\n",
    "        print(\"\\t ⚠️\\033[91m Veuillez choisir un reél\\033[0;37m \\n\")\n",
    "    except InvalidOperation:\n",
    "        print(\"\\t ⚠️\\033[91m Veuillez choisir un reél\\033[0;37m \\n\")\n",
    "  X0_0[1]=Decimal(X0_0[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "Ksidrys4AW-5"
   },
   "outputs": [],
   "source": [
    "def printParametersOP():\n",
    "  print(\"\\n\")\n",
    "  print(\"- Les paramétres utilisé sont: \")\n",
    "  print(\"\\t Le vecteur de départ X0= [\"+ str(X0_0[0])+\",\"+ str(X0_0[1])+\"]\")\n",
    "  print(\"\\t epsilon= \"+str(eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "05oAJpkIgLNs"
   },
   "outputs": [],
   "source": [
    "def difference(X, Y):\n",
    "  return sqrt(abs(X**2 - Y**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "ehJtFkkiz0c-"
   },
   "outputs": [],
   "source": [
    "\n",
    "def optimalLearningRate(function, X0):\n",
    "  X1=[0,0] # initialisation du vecteur\n",
    "\n",
    "  p = symbols(\"p\") # déclaration de la variable p\n",
    "  # calcule de φ0(p)\n",
    "  X1[0] = Decimal(str(X0[0])) - p* Decimal(str(f(X0[0],X0[1],grad[0] )))\n",
    "  X1[1] = Decimal(str(X0[1])) - p * Decimal(str(f(X0[0],X0[1],grad[1] )))\n",
    "  phi0 = f(X1[0],X1[1],function) # calcule de , φ0(ρ) = f(X0 − ρ∇f(X0))\n",
    "  phi0= simplify(phi0)\n",
    "  diff_phi0=partial_derivative(p,phi0) # calcule de φ0(p)'\n",
    "  diff_phi0= simplify(diff_phi0)\n",
    " \n",
    "  # φ0\"(ρ) > 0 --> c'est une fonction convexe\n",
    "  #if(partial_derivative(p,diff_phi0)<0):\n",
    "  #  print(\"\\t ⚠️\\033[91m La fonction φ n'est pas convexe donc elle ne peut pas atteindre son minimum globale\\033[0;37m \\n\")\n",
    "  #  Niveau1() # retourner au niveau1\n",
    "  if(diff_phi0==0):\n",
    "    return 0\n",
    "  # minimisation\n",
    "  else:\n",
    "    LR = eval(str(solve(diff_phi0)[0])) # résoudre l'équation φ0′(ρ) = 0\n",
    "    res=simplify(LR)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "0H0iCAT4m2OP"
   },
   "outputs": [],
   "source": [
    "def plotGradientDescentTable(nb_iterations,xIteration,yIteration,zIteration,pasIteration):\n",
    "  numIteration=list(range(nb_iterations+1))\n",
    "  data_matrix=[]\n",
    "  data_matrix.append([\"N° iteration\",\"X\",\"Y\",\"f(X,Y)\",\"Pas\"])\n",
    "  for i in range(nb_iterations+1):\n",
    "    data_matrix.append([numIteration[i],xIteration[i],yIteration[i],zIteration[i],pasIteration[i]])         \n",
    "  colorscale = [[0, '#9A169E'],[.5, '#efeaef'],[1, '#ffffff']]\n",
    "  fig =  ffactory.create_table(data_matrix,colorscale)\n",
    "  fig.update_layout(\n",
    "      template=\"plotly_dark\",\n",
    "      title_text = 'Tableau d’itérations',\n",
    "      margin = {'t':50},\n",
    "  )\n",
    "  fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "v0dKsm9Rzk_O"
   },
   "outputs": [],
   "source": [
    "def gradientWithOptimalLearningRateCalcul(function):\n",
    "\n",
    "  # initialisation\n",
    "  global X0\n",
    "  global xit_optimal\n",
    "  global yit_optimal\n",
    "  global zit_optimal\n",
    "  global iterationsWithOptimalLR # liste d'ittérations\n",
    "  global nb_iterations_optimal # nb d'ittérations\n",
    "  global Pas_optimal\n",
    "  X0=deepcopy(X0_0)\n",
    "  xit_optimal=[] # liste des abscisses\n",
    "  yit_optimal=[] # liste des ordonnées\n",
    "  zit_optimal=[] # list des résultats de la fonction après remplacement par le vecteur calculer\n",
    "  interationNumber=[] # initialiser la liste du nb d'ittérations\n",
    "  Pas_optimal=[]\n",
    "  #print(\"-------------------------------------------------------------------------------------------------------------------\")\n",
    "  #print(\"|     N° iteration     |          X           |          Y           |       f(X,Y)         |         Pas          |\")\n",
    "  #print(\"-------------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "  calculGradientVector(function) # calcule du vecteur gradient\n",
    "  iterationsWithOptimalLR=[] # initialisation  de la liste des ittérations\n",
    "  nb_max_iterations = 3000 # Nb max d'iteration\n",
    "  nb_iterations_optimal = 0  #initialisation du nbre d'itération\n",
    "\n",
    "  Z0 = Decimal(str(f(X0[0],X0[1],function))) # calcule de la fonction en remplçant x et y par le vecteur départ\n",
    "  iterationsWithOptimalLR.append([X0[0],X0[1]]) # sauvegarder la valeur du point de départ\n",
    "  interationNumber.append(\"Trace \"+str(nb_iterations_optimal)) # sauvegarder la trace\n",
    "  #print(\"|\"+\" \"*((22-len(str(nb_iterations_optimal)))//2)+\"\"+str(nb_iterations_optimal)+\" \"*(22-len(str(nb_iterations_optimal))-(22-len(str(nb_iterations_optimal)))//2)+\"|\"+\" \"*((22-len(str(X0[0])))//2)+\"\"+str(X0[0])+\" \"*(22-len(str(X0[0]))-(22-len(str(X0[0])))//2)+\"|\"+\" \"*((22-len(str(X0[1])))//2)+\"\"+str(X0[1])+\" \"*(22-len(str(X0[1]))-(22-len(str(X0[1])))//2)+\"|\"+\" \"*((22-len(str(Z0)))//2)+\"\"+str(Z0)+\" \"*(22-len(str(Z0))-(22-len(str(Z0)))//2)+\"|\"+\" \"*((22-len(str('-')))//2)+\"\"+str('-')+\" \"*(22-len(str('-'))-(22-len(str('-')))//2)+\"|\")\n",
    "\n",
    "  xit_optimal.append(Decimal(str(X0[0]))) # sauvegarder l'abscisse du point\n",
    "  yit_optimal.append(Decimal(str(X0[1]))) # sauvegarder l'ordonné du point\n",
    "  zit_optimal.append(Decimal(str(Z0))) # sauvegarder le résultat de la fonction en remplçant x et y par le vecteur départ\n",
    "  Pas_optimal.append('-')\n",
    "  X1=[0,0]\n",
    "  LR = optimalLearningRate(function,X0) # chercher le pas optimal\n",
    "  # xk+1=xk+alpha*dk\n",
    "  X1[0] = Decimal(str(X0[0])) - LR * Decimal(str(f(X0[0],X0[1],grad[0]))) # calcule de l'abscisse du nouveau vecteur\n",
    "  X1[1] = Decimal(str(X0[1])) - LR * Decimal(str(f(X0[0],X0[1],grad[1]))) # calcule de l'ordonnée du nouveau vecteur\n",
    "  Z0_tmp = Decimal(str(f(X1[0],X1[1],function))) # calcule de la fonction avec le nouveau vecteur \n",
    "  Pas_optimal.append(str(LR))\n",
    "  nb_iterations_optimal = nb_iterations_optimal + 1 # incrémenter le nbr d'ittératioon\n",
    "  iterationsWithOptimalLR.append([X1[0],X1[1]]) # sauvergarder la nouvelle valeur du vecteur\n",
    "  #print(\"|\"+\" \"*((22-len(str(nb_iterations_optimal)))//2)+\"\"+str(nb_iterations_optimal)+\" \"*(22-len(str(nb_iterations_optimal))-(22-len(str(nb_iterations_optimal)))//2)+\"|\"+\" \"*((22-len(str(X1[0])))//2)+\"\"+str(X1[0])+\" \"*(22-len(str(X1[0]))-(22-len(str(X1[0])))//2)+\"|\"+\" \"*((22-len(str(X1[1])))//2)+\"\"+str(X1[1])+\" \"*(22-len(str(X1[1]))-(22-len(str(X1[1])))//2)+\"|\"+\" \"*((22-len(str(Z0_tmp)))//2)+\"\"+str(Z0_tmp)+\" \"*(22-len(str(Z0_tmp))-(22-len(str(Z0_tmp)))//2)+\"|\"+\" \"*((22-len(str(LR)))//2)+\"\"+str(LR)+\" \"*(22-len(str(LR))-(22-len(str(LR)))//2)+\"|\")\n",
    "\n",
    "  xit_optimal.append(Decimal(str(X1[0]))) # sauvegarder l'abscisse du point\n",
    "  yit_optimal.append(Decimal(str(X1[1]))) # sauvegarder l'ordonné du point\n",
    "  zit_optimal.append(Decimal(str(Z0_tmp))) # sauvegarder le résultat de la fonction en remplçant x et y par le nouveau vecteur Xk+1\n",
    "  interationNumber.append(\"Trace \"+str(nb_iterations_optimal)) # sauvegarder la trace\n",
    "\n",
    "\n",
    "  # ||Z0_tmp - Z0||² > eps signifie si la différence entre les deux points < epsilone ce qui implique que le fct a convergé\n",
    "  # vérifier si la ||Z0_tmp - Z0||² et si on a dépassé le nombre d'ittération maximale\n",
    "  while ((difference(Z0_tmp,Z0) > eps) and (nb_iterations_optimal < nb_max_iterations)): \n",
    "    X0[0]=Decimal(str(X1[0])) # sauvegarder l'ancienne valeur de l'abscisse\n",
    "    X0[1]=Decimal(str(X1[1])) # sauvegarder l'ancienne valeur de l'ordonné\n",
    "    Z0=Z0_tmp # sauvegarder le résultat de la fonction avec l'ancienne valeur du vecteur\n",
    "    LR = optimalLearningRate(function,X0) # chercher le pas optimal\n",
    "    # xk+1=xk+alpha*dk\n",
    "    X1[0] = X0[0] - LR * f(X0[0],X0[1],grad[0] ) # calcule de l'abscisse du nouveau vecteur Xk+1\n",
    "    X1[1] = X0[1] - LR * f(X0[0],X0[1],grad[1] ) # calcule de l'ordonnée du nouveau vecteur Xk+1\n",
    "    \n",
    "    \n",
    "    Z0_tmp = f(X1[0],X1[1],function)  # calcule de la fonction avec le nouveau vecteur \n",
    "    nb_iterations_optimal = nb_iterations_optimal + 1 # incrémenter le nbr d'ittératioon\n",
    "    iterationsWithOptimalLR.append([X1[0],X1[1]]) # sauvergarder la nouvelle valeur du vecteur\n",
    "    interationNumber.append(\"Trace \"+str(nb_iterations_optimal)) # sauvegarder la trace\n",
    "\n",
    "    xit_optimal.append(Decimal(str(X1[0]))) # sauvegarder l'abscisse du point\n",
    "    yit_optimal.append(Decimal(str(X1[1]))) # sauvegarder l'ordonné du point\n",
    "    zit_optimal.append(Decimal(str(Z0_tmp))) # sauvegarder le résultat de la fonction en remplçant x et y par le nouveau vecteur Xk+1\n",
    "    Pas_optimal.append(str(LR))\n",
    "   # print(\"|\"+\" \"*((22-len(str(nb_iterations_optimal)))//2)+\"\"+str(nb_iterations_optimal)+\" \"*(22-len(str(nb_iterations_optimal))-(22-len(str(nb_iterations_optimal)))//2)+\"|\"+\" \"*((22-len(str(X1[0])))//2)+\"\"+str(X1[0])+\" \"*(22-len(str(X1[0]))-(22-len(str(X1[0])))//2)+\"|\"+\" \"*((22-len(str(X1[1])))//2)+\"\"+str(X1[1])+\" \"*(22-len(str(X1[1]))-(22-len(str(X1[1])))//2)+\"|\"+\" \"*((22-len(str(Z0_tmp)))//2)+\"\"+str(Z0_tmp)+\" \"*(22-len(str(Z0_tmp))-(22-len(str(Z0_tmp)))//2)+\"|\"+\" \"*((22-len(str(LR)))//2)+\"\"+str(LR)+\" \"*(22-len(str(LR))-(22-len(str(LR)))//2)+\"|\")\n",
    "  #print(\"-------------------------------------------------------------------------------------------------------------------\")\n",
    "  plotGradientDescentTable(nb_iterations_optimal,xit_optimal,yit_optimal,zit_optimal,Pas_optimal)\n",
    "  plotGradientDescent(xit_optimal,yit_optimal,zit_optimal,interationNumber)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "OVUPiBpO5Hmk"
   },
   "outputs": [],
   "source": [
    "def gradientWithOptimalLearningRate():\n",
    "  if (rep3==\"iii\"):\n",
    "    getParametersOp()\n",
    "  printParametersOP()\n",
    "  gradientWithOptimalLearningRateCalcul(function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwnYcaATFgsZ"
   },
   "source": [
    "### pas variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "6yi6hLgVGWW5"
   },
   "outputs": [],
   "source": [
    "def printParameters():\n",
    "  print(\"\\t Le vecteur de départ X0= [\"+ str(X0_0[0])+\",\"+ str(X0_0[1])+\"]\")\n",
    "  print(\"\\t epsilon= \"+str(eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "rUSlkdRyF5EZ"
   },
   "outputs": [],
   "source": [
    "def getLearningRate():\n",
    "  testLearning_rate=1\n",
    "  # Lecture de learning_rate\n",
    "  lr = 0;\n",
    "  while(testLearning_rate):\n",
    "    try:\n",
    "        print(\"\\t ✅ \\033[0;32m\\Recommendation:\\033[0;37m\")\n",
    "        print(\"\\t Si le pas est trop grand, alors vous ferez de trop grands pas dans la descente de gradient. Cela a l’avantage de descendre rapidement vers le minimum de la fonction, mais vous risquez de louper ce minimum en oscillant autour à l’infini\")\n",
    "        print(\"\\t Si le pas très faible, alors vous risquez de mettre un temps infini avant de converger vers le minimum de la fonction.\")\n",
    "        lr = float(input(\"\\t Tapez la valeur de pas = \"))\n",
    "        testLearning_rate=0\n",
    "        if(learning_rate<=0):\n",
    "          print(\"\\t ⚠️\\033[91m Veuillez choisir un reél positif\\033[0;37m \\n\")\n",
    "          testLearning_rate=1\n",
    "    except ValueError:\n",
    "        print(\"\\t ⚠️\\033[91m Veuillez choisir un reél\\033[0;37m \\n\")\n",
    "          \n",
    "  return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "rrnpFujFyNEZ"
   },
   "outputs": [],
   "source": [
    "def gradientWithVariableLearningRateCalcule(function):\n",
    "\n",
    "  # initialisation\n",
    "  global X0\n",
    "  global xit_var\n",
    "  global yit_var\n",
    "  global zit_var\n",
    "  global iterationsWithVariableLR # liste des ittérations \n",
    "  global nb_iterations_var # nb d'ittération\n",
    "  global Pas_variable\n",
    "  X0=deepcopy(X0_0)\n",
    "  xit_var=[] # liste des abscisses\n",
    "  yit_var=[] # liste des ordonnées\n",
    "  zit_var=[] # list des résultats de la fonction après remplacement par le vecteur calculer\n",
    "  interationNumber=[] # initialiser la liste du nb d'ittérations\n",
    "  Pas_variable=[]\n",
    "  #print(\"-------------------------------------------------------------------------------------------------------------------\")\n",
    "  #print(\"|     N° iteration     |          X           |          Y           |       f(X,Y)         |         Pas          |\")\n",
    "  #print(\"-------------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "  calculGradientVector(function) # calcule du vecteur gradient\n",
    "  iterationsWithVariableLR=[] # initialisation  de la liste des ittérations\n",
    "  nb_max_iterations = 3000 # Nb max d'iteration\n",
    "  nb_iterations_var = 0  #initialisation du nbre d'itération\n",
    "\n",
    "  Z0 = f(X0[0],X0[1],function) # calcule de la fonction en remplçant x et y par le vecteur départ\n",
    "  iterationsWithVariableLR.append([X0[0],X0[1]]) # sauvegarder la valeur du point de départ\n",
    "  interationNumber.append(\"Trace \"+str(nb_iterations_var)) # sauvegarder la trace\n",
    "  #print(\"|\"+\" \"*((22-len(str(nb_iterations_var)))//2)+\"\"+str(nb_iterations_var)+\" \"*(22-len(str(nb_iterations_var))-(22-len(str(nb_iterations_var)))//2)+\"|\"+\" \"*((22-len(str(X0[0])))//2)+\"\"+str(X0[0])+\" \"*(22-len(str(X0[0]))-(22-len(str(X0[0])))//2)+\"|\"+\" \"*((22-len(str(X0[1])))//2)+\"\"+str(X0[1])+\" \"*(22-len(str(X0[1]))-(22-len(str(X0[1])))//2)+\"|\"+\" \"*((22-len(str(Z0)))//2)+\"\"+str(Z0)+\" \"*(22-len(str(Z0))-(22-len(str(Z0)))//2)+\"|\"+\" \"*((22-len(str('-')))//2)+\"\"+str('-')+\" \"*(22-len(str('-'))-(22-len(str('-')))//2)+\"|\")\n",
    "\n",
    "  xit_var.append(Decimal(str(X0[0]))) # sauvegarder l'abscisse du point\n",
    "  yit_var.append(Decimal(str(X0[1]))) # sauvegarder l'ordonné du point\n",
    "  zit_var.append(Decimal(str(Z0))) # sauvegarder le résultat de la fonction en remplçant x et y par le vecteur départ\n",
    "  Pas_variable.append('-')\n",
    "  X1=[0,0]\n",
    "  LR = Decimal(str(getLearningRate())) # chercher le pas optimal\n",
    "\n",
    "  # xk+1=xk+alpha*dk\n",
    "  X1[0] = X0[0] - LR * f(X0[0],X0[1],grad[0]) # calcule de l'abscisse du nouveau vecteur\n",
    "  X1[1] = X0[1] - LR * f(X0[0],X0[1],grad[1]) # calcule de l'ordonnée du nouveau vecteur\n",
    "  Z0_tmp = f(X1[0],X1[1],function) # calcule de la fonction avec le nouveau vecteur \n",
    "\n",
    "  nb_iterations_var = nb_iterations_var + 1 # incrémenter le nbr d'ittératioon\n",
    "  iterationsWithVariableLR.append([X1[0],X1[1]]) # sauvergarder la nouvelle valeur du vecteur\n",
    "  #print(\"|\"+\" \"*((22-len(str(nb_iterations_var)))//2)+\"\"+str(nb_iterations_var)+\" \"*(22-len(str(nb_iterations_var))-(22-len(str(nb_iterations_var)))//2)+\"|\"+\" \"*((22-len(str(X1[0])))//2)+\"\"+str(X1[0])+\" \"*(22-len(str(X1[0]))-(22-len(str(X1[0])))//2)+\"|\"+\" \"*((22-len(str(X1[1])))//2)+\"\"+str(X1[1])+\" \"*(22-len(str(X1[1]))-(22-len(str(X1[1])))//2)+\"|\"+\" \"*((22-len(str(Z0_tmp)))//2)+\"\"+str(Z0_tmp)+\" \"*(22-len(str(Z0_tmp))-(22-len(str(Z0_tmp)))//2)+\"|\"+\" \"*((22-len(str(LR)))//2)+\"\"+str(LR)+\" \"*(22-len(str(LR))-(22-len(str(LR)))//2)+\"|\")\n",
    "\n",
    "  xit_var.append(Decimal(str(X1[0]))) # sauvegarder l'abscisse du point\n",
    "  yit_var.append(Decimal(str(X1[1]))) # sauvegarder l'ordonné du point\n",
    "  zit_var.append(Decimal(str(Z0_tmp))) # sauvegarder le résultat de la fonction en remplçant x et y par le nouveau vecteur Xk+1\n",
    "  interationNumber.append(\"Trace \"+str(nb_iterations_var)) # sauvegarder la trace\n",
    "  Pas_variable.append(str(LR))\n",
    "\n",
    "  # ||Z0_tmp - Z0||² > eps signifie si la différence entre les deux points < epsilone ce qui implique que le fct a convergé\n",
    "  # vérifier si la ||Z0_tmp - Z0||² et si on a dépassé le nombre d'ittération maximale\n",
    "  while ((difference(Z0_tmp,Z0) > eps) and (nb_iterations_var < nb_max_iterations)): \n",
    "    X0[0]=Decimal(X1[0]) # sauvegarder l'ancienne valeur de l'abscisse\n",
    "    X0[1]=Decimal(X1[1]) # sauvegarder l'ancienne valeur de l'ordonné\n",
    "    Z0=Decimal(Z0_tmp) # sauvegarder le résultat de la fonction avec l'ancienne valeur du vecteur\n",
    "    LR = Decimal(getLearningRate()) # chercher le pas optimal\n",
    "    # xk+1=xk+alpha*dk\n",
    "    X1[0] = X0[0] - LR * f(X0[0],X0[1],grad[0] ) # calcule de l'abscisse du nouveau vecteur Xk+1\n",
    "    X1[1] = X0[1] - LR * f(X0[0],X0[1],grad[1] ) # calcule de l'ordonnée du nouveau vecteur Xk+1\n",
    "    \n",
    "    \n",
    "    Z0_tmp = f(X1[0],X1[1],function)  # calcule de la fonction avec le nouveau vecteur \n",
    "    nb_iterations_var = nb_iterations_var + 1 # incrémenter le nbr d'ittératioon\n",
    "    iterationsWithVariableLR.append([X1[0],X1[1]]) # sauvergarder la nouvelle valeur du vecteur\n",
    "    interationNumber.append(\"Trace \"+str(nb_iterations_var)) # sauvegarder la trace\n",
    "\n",
    "    xit_var.append(Decimal(str(X1[0]))) # sauvegarder l'abscisse du point\n",
    "    yit_var.append(Decimal(str(X1[1]))) # sauvegarder l'ordonné du point\n",
    "    zit_var.append(Decimal(str(Z0_tmp))) # sauvegarder le résultat de la fonction en remplçant x et y par le nouveau vecteur Xk+1\n",
    "    Pas_variable.append(str(LR))\n",
    "   # print(\"|\"+\" \"*((22-len(str(nb_iterations_var)))//2)+\"\"+str(nb_iterations_var)+\" \"*(22-len(str(nb_iterations_var))-(22-len(str(nb_iterations_var)))//2)+\"|\"+\" \"*((22-len(str(X1[0])))//2)+\"\"+str(X1[0])+\" \"*(22-len(str(X1[0]))-(22-len(str(X1[0])))//2)+\"|\"+\" \"*((22-len(str(X1[1])))//2)+\"\"+str(X1[1])+\" \"*(22-len(str(X1[1]))-(22-len(str(X1[1])))//2)+\"|\"+\" \"*((22-len(str(Z0_tmp)))//2)+\"\"+str(Z0_tmp)+\" \"*(22-len(str(Z0_tmp))-(22-len(str(Z0_tmp)))//2)+\"|\"+\" \"*((22-len(str(LR)))//2)+\"\"+str(LR)+\" \"*(22-len(str(LR))-(22-len(str(LR)))//2)+\"|\")\n",
    "  #print(\"-------------------------------------------------------------------------------------------------------------------\")\n",
    "  plotGradientDescentTable(nb_iterations_var,xit_var,yit_var,zit_var,Pas_variable)\n",
    "  plotGradientDescent(xit_var,yit_var,zit_var,interationNumber)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "c8CP-Wqh4yKb"
   },
   "outputs": [],
   "source": [
    "def gradientWithVariableLearningRate():\n",
    "  if (rep3==\"ii\"):\n",
    "    getParameters()\n",
    "  printParameters()\n",
    "  gradientWithVariableLearningRateCalcule(function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYx9xDJAYiDD"
   },
   "source": [
    "### Comparaison entre les 3 méthodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "13S7r9xTqYsc"
   },
   "outputs": [],
   "source": [
    "def comparaison():\n",
    "  print(\"Function:\")\n",
    "  data_matrix = [['Méthode', ' Nombre d’itérations'],\n",
    "               ['Méthode de gradient à pas fixe', nb_iterations_fixed],\n",
    "               ['Méthode de gradient à pas variable', nb_iterations_var],\n",
    "               ['Méthode de gradient à pas optimal ', nb_iterations_optimal],\n",
    "               ]    \n",
    "\n",
    "  colorscale = [[0, '#9A169E'],[.5, '#efeaef'],[1, '#ffffff']]\n",
    "  fig =  ffactory.create_table(data_matrix,colorscale)\n",
    "  fig.update_layout(\n",
    "      template=\"plotly_dark\",\n",
    "      title_text = 'Comparaison entre les 3 méthodes de gradient pour:<br>     f(x,y)='+function,\n",
    "      margin = {'t':50},\n",
    "  )\n",
    "  fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_g_nLyIXxxX"
   },
   "source": [
    "## Niveau 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "J6kXxYX7pIr3"
   },
   "outputs": [],
   "source": [
    "def Niveau1():\n",
    "  global rep\n",
    "  global fonctionDisplay\n",
    "  global fonctioninput\n",
    "  print(\"\\t ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\")\n",
    "  print(\"\\t ¤                Bienvenue  Gradient Descent               ¤\")\n",
    "  print(\"\\t ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\")\n",
    "  print(\"\\n\");\n",
    "  rep=0\n",
    "  while (rep!=1 and rep!=2 and rep!= 3):\n",
    "    print(\"\\t ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤  Menu ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\\n\")\n",
    "    print(\"\\t           1-Choisir une fonction de la mémoire              \")\n",
    "    print(\"\\t           2-Saisir sa propre fonction.                      \")\n",
    "    print(\"\\t           3-Quitter                                         \\n\")\n",
    "    print(\"\\t ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\\n\")\n",
    "    rep = input(\"\\t Option : \")\n",
    "    if(not(rep.isnumeric()) ):\n",
    "      print(\"\\t ⚠️\\033[91m Veuillez choisir un entier égale à  1 ou 2 ou 3 \\033[0;37m \\n\")\n",
    "    else:\n",
    "      rep=int(rep)\n",
    "      if ((rep!=1 and rep!=2 and rep!= 3)):\n",
    "        print(\"\\t ⚠️\\033[91m Veuillez choisir un entier égale à  1 ou 2 ou 3 \\033[0;37m \\n\")\n",
    "  print(\"\\n\")\n",
    "  if (rep==1):\n",
    "    print(\"\\t ¤¤¤¤¤¤¤¤¤¤¤¤  Choisir une fonction de la mémoire ¤¤¤¤¤¤¤¤¤¤¤¤\\n\")\n",
    "    fonctionDisplay=localFunctions()\n",
    "    display(fonctionDisplay)\n",
    "    \n",
    "  elif (rep==2):\n",
    "    print(\"\\t ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤  Saisir une fonction ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\\n\")\n",
    "    restrictionInputFunction()\n",
    "    fonctioninput=givenFunctions()\n",
    "    while(not restrictionRespected(fonctioninput)):\n",
    "      print(\"\\t ⚠️\\033[91m Veuillez tapez une fonction valide \\033[0;37m\")\n",
    "      fonctioninput=givenFunctions()\n",
    "  else:\n",
    "    print(\"\\t ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤  Au revoir ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXfpUSWQX1Mv"
   },
   "source": [
    "## Niveau 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "dm85RqaTwGdy"
   },
   "outputs": [],
   "source": [
    "def Niveau2():\n",
    "  rep2 = \"x\";\n",
    "  while (rep2!=\"a\" and rep2!=\"b\" and rep2!= \"c\" and rep2!= \"d\" and rep2!= \"e\" and rep2!= \"f\"):\n",
    "    print(\"\\t ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤ Sous menu ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\\n\")\n",
    "    print(\"\\t           a- Tracer la courbe    \")\n",
    "    print(\"\\t           b- Tracer les lignes de niveaux et les ajouter au graphe existant \")\n",
    "    print(\"\\t           c- Calculer le vecteur gradient    \")\n",
    "    print(\"\\t           d- Calculer la matrice Hessienne \")\n",
    "    print(\"\\t           e- Appliquer la méthode de gradient  \")\n",
    "    print(\"\\t           f- Revenir au niveau 1            \")\n",
    "    print(\"\\t ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\\n\")\n",
    "    rep2 = input(\"\\t Option : \")\n",
    "    if(rep2.isnumeric() ):\n",
    "      print(\"\\t ⚠️\\033[91m Veuillez choisir un caractére de a jusqu'a f \\033[0;37m \\n\")\n",
    "    else:\n",
    "      if ((rep2!=\"a\" and rep2!=\"b\" and rep2!= \"c\" and rep2!= \"d\" and rep2!= \"e\" and rep2!= \"f\")):\n",
    "        print(\"\\t ⚠️\\033[91m Veuillez choisir un caractére de a jusqu'a f \\033[0;37m \\n\")\n",
    "  print(\"\\n\")\n",
    "  if (rep2==\"a\"):\n",
    "    plotFunction(function)\n",
    "  elif (rep2==\"b\"):\n",
    "    plotFunctionWithCounter(function)\n",
    "  elif (rep2==\"c\"):\n",
    "    calculGradientVector(function)\n",
    "    print(\"Le vecteur gradient de la fonction {0} est :\".format(function))\n",
    "    pprint(grad)\n",
    "  elif (rep2==\"d\"):\n",
    "    calculHassienneMatrix(function)\n",
    "  elif (rep2==\"e\"):\n",
    "    Niveau3()\n",
    "  elif (rep2==\"f\"):\n",
    "    Niveau1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBPC-Zj8X4sv"
   },
   "source": [
    "## Niveau 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "oMrRyV4hCeqw"
   },
   "outputs": [],
   "source": [
    "def Niveau3():\n",
    "  global rep3\n",
    "  TimerOfFixedLearningRate=0\n",
    "  TimerOfVariableLearningRate=0\n",
    "  TimerOfOptimalLearningRate=0\n",
    "  rep3 = \"x\";\n",
    "  #if(isConvex(function) != True):\n",
    "   #  print(\"\\t ⚠️\\033[91m Votre fonction n'est pas convexe donc nous ne pouvons pas appliquer la méthode de gradient \\033[0;37m \\n\")\n",
    "   #  Niveau1()\n",
    "\n",
    "  while (rep3!=\"i\" and rep3!=\"ii\" and rep3!= \"iii\" and rep3!= \"iv\"):\n",
    "    print(\"\\t ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤ Appliquer la méthode de gradient ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\\n\")\n",
    "    print(\"\\t           i- Appliquer la méthode de gradient à pas fixe    \")\n",
    "    print(\"\\t           ii- Appliquer la méthode de gradient à pas variable\")\n",
    "    print(\"\\t           iii- Appliquer la méthode de gradient à pas optimal   \")\n",
    "    print(\"\\t           iv- Afficher un comparatif des trois méthodes de gradients\")\n",
    "    print(\"\\t ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\\n\")\n",
    "    rep3 = input(\"\\t Option : \")\n",
    "    if(rep3.isnumeric() ):\n",
    "      print(\"\\t ⚠️\\033[91m Veuillez choisir soit i ou ii ou iii ou iv \\033[0;37m \\n\")\n",
    "    else:\n",
    "      if ((rep3!=\"i\" and rep3!=\"ii\" and rep3!= \"iii\" and rep3!= \"iv\")):\n",
    "        print(\"\\t ⚠️\\033[91m Veuillez choisir soit i ou ii ou iii ou iv \\033[0;37m \\n\")\n",
    "  print(\"\\n\")\n",
    "  if (rep3==\"i\"):\n",
    "    gradientWithFixedLearningRate()\n",
    "  elif (rep3==\"ii\"):\n",
    "    gradientWithVariableLearningRate()\n",
    "  elif (rep3==\"iii\"):\n",
    "    gradientWithOptimalLearningRate()\n",
    "  elif (rep3==\"iv\"):\n",
    "    print(\"\\n\")\n",
    "    print(\"\\033[1m\\033[0;35m * Appliquer la méthode de gradient à pas fixe\\033[0;37m\\033[0m \\n\")\n",
    "    gradientWithFixedLearningRate()\n",
    "    print(\"\\n\")\n",
    "    print(\"\\033[1m\\033[0;35m * Appliquer la méthode de gradient à pas variable\\033[0;37m\\033[0m\")\n",
    "    gradientWithVariableLearningRate()\n",
    "    print(\"\\n\")\n",
    "    print(\"\\033[1m\\033[0;35m * Appliquer la méthode de gradient à pas optimal\\033[0;37m\\033[0m\")\n",
    "    gradientWithOptimalLearningRate()\n",
    "    print(\"\\033[1m\\033[0;35m * Conclusion \\033[0;37m\\033[0m\")\n",
    "    comparaison()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U2edg7_Pupp"
   },
   "source": [
    "# Exécution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432,
     "referenced_widgets": [
      "fb035ee18809422da0ce871744c9de35",
      "00601bc777954fe79e4a19c906cda47f",
      "9df184e1c9f245c48dc10aec32a497ef"
     ]
    },
    "id": "VodsWIVkn1CL",
    "outputId": "75b263de-42c7-4eae-c564-cbf5c9e5db89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
      "\t ¤                Bienvenue  Gradient Descent               ¤\n",
      "\t ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
      "\n",
      "\n",
      "\t ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤  Menu ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
      "\n",
      "\t           1-Choisir une fonction de la mémoire              \n",
      "\t           2-Saisir sa propre fonction.                      \n",
      "\t           3-Quitter                                         \n",
      "\n",
      "\t ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
      "\n",
      "\t Option : 1\n",
      "\n",
      "\n",
      "\t ¤¤¤¤¤¤¤¤¤¤¤¤  Choisir une fonction de la mémoire ¤¤¤¤¤¤¤¤¤¤¤¤\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa1b4520fc2404abb60a4d520e3da0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Select(description='Choisir une fonction:', options=('(1 - x)**2 + 100 * (y - x**2)**2', 'x*exp(-x**2-y**2)', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Niveau1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Zp8KLJSFu3d",
    "outputId": "85754c84-ba17-4b0d-a169-efa22e14a598"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La fonction choisie est :\n",
      "                         2\n",
      "       2       ⎛   2    ⎞ \n",
      "(1 - x)  + 100⋅⎝- x  + y⎠ \n"
     ]
    }
   ],
   "source": [
    "if rep==1:\n",
    "  function=fonctionDisplay.value\n",
    "elif rep==2:\n",
    "  function=fonctioninput\n",
    "print(\"La fonction choisie est :\")\n",
    "pprint(ff(x,y,function))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "w7noDj3moYrl",
    "outputId": "8fe0c88b-799d-4b81-923f-bcc6524160c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤ Sous menu ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
      "\n",
      "\t           a- Tracer la courbe    \n",
      "\t           b- Tracer les lignes de niveaux et les ajouter au graphe existant \n",
      "\t           c- Calculer le vecteur gradient    \n",
      "\t           d- Calculer la matrice Hessienne \n",
      "\t           e- Appliquer la méthode de gradient  \n",
      "\t           f- Revenir au niveau 1            \n",
      "\t ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
      "\n",
      "\t Option : e\n",
      "\n",
      "\n",
      "\t ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤ Appliquer la méthode de gradient ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
      "\n",
      "\t           i- Appliquer la méthode de gradient à pas fixe    \n",
      "\t           ii- Appliquer la méthode de gradient à pas variable\n",
      "\t           iii- Appliquer la méthode de gradient à pas optimal   \n",
      "\t           iv- Afficher un comparatif des trois méthodes de gradients\n",
      "\t ¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
      "\n",
      "\t Option : i\n",
      "\n",
      "\n",
      "\t ✅ \u001b[0;32mRecommendation:\u001b[0;37m\n",
      "\t Epsilon sert à symboliser un nombre ou une quantité extrêmement petite\n",
      "\t Tapez la valeur de epsilon = 0.001\n",
      "\t ✅ \u001b[0;32m\\Recommendation:\u001b[0;37m\n",
      "\t Si le pas est trop grand, alors vous ferez de trop grands pas dans la descente de gradient. Cela a l’avantage de descendre rapidement vers le minimum de la fonction, mais vous risquez de louper ce minimum en oscillant autour à l’infini\n",
      "\t Si le pas très faible, alors vous risquez de mettre un temps infini avant de converger vers le minimum de la fonction.\n",
      "\t Tapez la valeur de pas = 0.001\n",
      "\t Tapez l’abscisse du vecteur de départ= 0.2\n",
      "\t ⚠️\u001b[91m Veuillez choisir une valeur entière\u001b[0;37m \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Niveau2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00601bc777954fe79e4a19c906cda47f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9df184e1c9f245c48dc10aec32a497ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fb035ee18809422da0ce871744c9de35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "SelectModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SelectModel",
      "_options_labels": [
       "(1 - x)**2 + 100 * (y - x**2)**2",
       "x*exp(-x**2-y**2)",
       "x**2 + y**2"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "SelectView",
      "description": "Choisir une fonction:",
      "description_tooltip": null,
      "disabled": false,
      "index": 2,
      "layout": "IPY_MODEL_00601bc777954fe79e4a19c906cda47f",
      "rows": 5,
      "style": "IPY_MODEL_9df184e1c9f245c48dc10aec32a497ef"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
